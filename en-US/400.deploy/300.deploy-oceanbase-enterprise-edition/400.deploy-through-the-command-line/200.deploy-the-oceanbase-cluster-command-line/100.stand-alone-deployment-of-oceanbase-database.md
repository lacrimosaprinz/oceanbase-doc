# Deploy a standalone OceanBase database by using the CLI

The standalone mode is a streamlined database architecture of OceanBase Database. Compared with a distributed cluster, a standalone database has only one zone that contains only one OBServer node. However, without multiple replicas and scaling capabilities, a standalone database is suitable only for development, testing, and business systems that do not require high data security.

This topic describes how to deploy a standalone OceanBase database by using the CLI.

## Considerations

If the database to deploy requires resource isolation, you must first configure cgroups before you deploy the database.

For more information about resource isolation and cgroup, see [Resource isolation](../../../../600.manage/200.tenant-management/600.common-tenant-operations/300.resource-isolation/100.resource-isolation-overview.md) and [Configure cgroups](../../../../600.manage/200.tenant-management/600.common-tenant-operations/300.resource-isolation/200.resource-isolation-of-oracle-mode/100.config-cgroups-of-oracle-mode.md).

## Prerequisites

Before you install OceanBase Database, make sure that:

* The OBServer node has been configured. For more information, see [Configure servers](../../200.preparations-before-deploy/200.configure-servers.md) and [Initialize an OBServer node by using oatcli](../100.configure-a-deployment-environment-command-line/100.initializing-the-observer-server-using-oatcli.md).

* You have obtained the RPM package of OceanBase Database. For more information, see [Prepare installation packages](../../200.preparations-before-deploy/300.prepare-installation-packages.md).

## Procedure

### Step 1: Install the RPM package

1. Install the RPM package for OceanBase Database.

   ```shell
   [root@xxx /]# cd $rpm_dir
   [root@xxx $rpm_dir]# rpm -ivh $rpm_name
   ```

   Here, `$rpm_dir` specifies the directory in which the RPM package is stored, and `$rpm_name` specifies the name of the RPM package.

   <main id="notice" type='explain'>
      <h4>Note</h4>
      <p>By default, OceanBase Database is installed in the <code>/home/admin/oceanbase</code> directory. </p>
   </main>

   Here is an example:

   ```shell
   [root@xxx /home/admin/rpm]# rpm -ivh oceanbase-4.2.0.0-100000052023073123.el7.x86_64.rpm
   Preparing...                          ################################# [100%]
   Updating / installing...
      1:oceanbase-4.2.0.0-100000052023073################################# [100%]
   ```

2. (Optional) Install OceanBase Client (OBClient).

   OBClient is a command-line client tool dedicated to OceanBase Database. You can use OBClient to connect to MySQL and Oracle tenants of OceanBase Database. You can also use the mysql client to connect to MySQL tenants of OceanBase Database.

   <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>OBClient of a version earlier than V2.2.1 depends on OceanBase Connector/C. Therefore, you must first install OceanBase Connector/C as needed. </br>Contact OceanBase Technical Support to obtain the RPM package of OBClient. If the OBClient version is earlier than V2.2.1, you must also obtain the RPM package of OceanBase Connector/C. </p>
   </main>

   Here is an example:

   ```shell
   [root@xxx /home/admin/rpm]# rpm -ivh obclient-2.2.1-20221122151945.el7.alios7.x86_64.rpm
   Preparing...                          ################################# [100%]
   Updating / installing...
      1:obclient-2.2.1-20221122151945.el7################################# [100%]

   ## Verify that the installation is successful. ##
   [root@xxx /home/admin/rpm]# which obclient
   /usr/bin/obclient
   ```

### Step 2: Configure directories

1. (Optional) Clear the OceanBase Database directory.

   You do not need to clear the directory if you deploy OceanBase Database on the server for the first time.

   You can directly clear the old OceanBase Database directory in the following cases:

   * You want to clear the old OceanBase Database environment.
   * Problems occur during the installation and deployment process of OceanBase Database. The installation environment becomes disordered or files that will affect the next installation are generated.

   ```shell
   [root@xxx /home/admin]# rm -rf /data/1/$cluster_name
   [root@xxx /home/admin]# rm -rf /data/log1/$cluster_name
   [root@xxx /home/admin]# rm -rf /home/admin/oceanbase/store/$cluster_name
   [root@xxx /home/admin]# rm -rf /home/admin/oceanbase/log/* /home/admin/oceanbase/etc/*config*
   ```

   Here, `$cluster_name` specifies the cluster name.

   Here is an example:

   ```shell
   [root@xxx /home/admin]# rm -rf /data/1/obdemo
   [root@xxx /home/admin]# rm -rf /data/log1/obdemo
   [root@xxx /home/admin]# rm -rf /home/admin/oceanbase/store/obdemo
   [root@xxx /home/admin]# rm -rf /home/admin/oceanbase/log/* /home/admin/oceanbase/etc/*config*
   ```

2. Initialize the OceanBase Database directory.

   We recommend that you specify the data directory of OceanBase Database to an independent disk and link this directory to the home directory of OceanBase Database by using a soft link.

   ```shell
   ## Switch to the admin user. ##
   [root@xxx /home/admin]# su - admin

   ## Run the following command as the admin user. ##
   -bash-4.2$ mkdir -p /data/1/$cluster_name/{etc3,sstable,slog} 
   -bash-4.2$ mkdir -p /data/log1/$cluster_name/{clog,etc2} 
   -bash-4.2$ mkdir -p /home/admin/oceanbase/store/$cluster_name
   -bash-4.2$ for t in {etc3,sstable,slog};do ln -s /data/1/$cluster_name/$t /home/admin/oceanbase/store/$cluster_name/$t; done
   -bash-4.2$ for t in {clog,etc2};do ln -s /data/log1/$cluster_name/$t /home/admin/oceanbase/store/$cluster_name/$t; done
   ```

   Here, `$cluster_name` specifies the cluster name. Note that `slog` files and data files must be on the same disk.

   Here is an example:

   ```shell
   [root@xxx /home/admin]# su - admin
   -bash-4.2$ mkdir -p /data/1/obdemo/{etc3,sstable,slog}
   -bash-4.2$ mkdir -p /data/log1/obdemo/{clog,etc2}
   -bash-4.2$ mkdir -p /home/admin/oceanbase/store/obdemo
   -bash-4.2$ for t in {etc3,sstable,slog};do ln -s /data/1/obdemo/$t /home/admin/oceanbase/store/obdemo/$t; done
   -bash-4.2$ for t in {clog,etc2};do ln -s /data/log1/obdemo/$t /home/admin/oceanbase/store/obdemo/$t; done
   ```

   <main id="notice" type='explain'>
      <h4>Note</h4>
      <p>The <code>obdemo</code> directory is named after the cluster and can be modified. It is required when the process starts. </p>
   </main>

   **The result is as follows:**

   ```shell
   -bash-4.2$ cd /home/admin/oceanbase
   -bash-4.2$ tree store/
   store/
   `-- obdemo
      |-- clog -> /data/log1/obdemo/clog
      |-- etc2 -> /data/log1/obdemo/etc2
      |-- etc3 -> /data/1/obdemo/etc3
      |-- slog -> /data/1/obdemo/slog
      `-- sstable -> /data/1/obdemo/sstable

   6 directories, 0 files
   ```

### Step 3: Initialize OceanBase Database

<main id="notice" type='explain'>
   <h4>Note</h4>
   <p>The IP addresses in the sample code are for reference only. You need to enter the actual server IP address during deployment. </p>
</main>

1. Start the observer process.

   Start the observer process as the `admin` user by executing the following statement:

   ```shell
   cd /home/admin/oceanbase && /home/admin/oceanbase/bin/observer {-I $ip | -i $devname} -P $rpc_port -p $sql_port -z $zone_name -d /home/admin/oceanbase/store/$cluster_name -r '$ip:2882:2881' -c $cluster_id -n $cluster_name -o "system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/$cluster_name/etc3;/data/log1/$cluster_name/etc2"
   ```

   The parameters are described in the following table:

   | **Parameter** | **Description** |
   |--------|----------------------------|
   | `-I` \| `-i` | <ul><li>`-I`: the IP address of the node to start. If OceanBase Database is deployed in distributed mode, you cannot specify 127.0.0.1 as the IP address of the node to start. We recommend that you specify another IP address such as `-I 10.10.10.1`. </li><li>`-i`: the name of the NIC. You can run the `ifconfig` command to query the NIC name. </li></ul><main id="notice" type='explain'><h4>Note</h4><p>You can specify both the IP address and NIC name to start a node, such as `-I 10.10.10.1 -i eth0`. However, this is not recommended. </p></main> |
   | `-p` | The service port number, which is usually set to `2881`.  |
   | `-P` | The RPC port number, which is usually set to `2882`.  |
   | `-n` | The name of the cluster. It can be modified. Cluster names must be unique.  |
   | `-z` | The zone where the started observer process belongs.  |
   | `-d` | The primary directory of the cluster, which is created during initialization. Do not modify other parameters except `$cluster_name`.  |
   | `-c` | The cluster ID. It is a group of digits and can be modified. Cluster IDs must be unique.  |
   | `-l` | The log level.  |
   | `-r` | The RootService list in the `$ip:2882:2881` format. Separate multiple items with semicolons (;).  |
   | `-o` | The cluster startup parameters, which need to be specified as needed. <ul><li>`system_memory` specifies the internal reserved memory of OceanBase Database, which is `30G` by default. You can adjust this parameter to a smaller value in the case of insufficient memory on the server. The impact is that memory may be insufficient during performance tests. </li><li>`datafile_size` specifies the size of the `SSTable` data file of OceanBase Database, which is determined based on the available space of `/data/1/`. You no longer need to specify the size again once specified in this step. We recommend that you specify the data file size to at least `100G` and reserve some spaces. </li><li>`config_additional_dir` specifies the redundancy directory for the parameter file. </li></ul> |

   Here is an example:

   ```shell
   [root@xxx /home/admin]# su - admin
   -bash-4.2$ cd /home/admin/oceanbase && /home/admin/oceanbase/bin/observer -I 10.10.10.1 -P 2882 -p 2881 -z zone1 -d /home/admin/oceanbase/store/obdemo -r '10.10.10.1:2882:2881' -c 10001 -n obdemo -o "system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/obdemo/etc3;/data/log1/obdemo/etc2"
   ```

   You can use the following commands to check whether the observer process has started successfully:

   * Run the `netstat -ntlp` command. If ports `2881` and `2882` are listened to, the observer process is started.
   * Run the `ps -ef|grep observer` command to return information about the observer process.

   Here is an example:

   ```shell
   -bash-4.2$ netstat -ntlp
   (Not all processes could be identified, non-owned process info
   will not be shown, you would have to be root to see it all.)
   Active Internet connections (only servers)
   Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
   tcp        0      0 0.0.0.0:2881            0.0.0.0:*               LISTEN      11111/observer
   tcp        0      0 0.0.0.0:2882            0.0.0.0:*               LISTEN      11111/observer
   ...        ...    ...                       ...                     ...         ...

   -bash-4.2$ ps -ef|grep observer
   admin    11111      0 43 17:58 ?        00:00:14 /home/admin/oceanbase/bin/observer -I 10.10.10.1 -P 2882 -p 2881 -z zone1 -d /home/admin/oceanbase/store/obdemo -r 10.10.10.1:2882:2881 -c 10001 -n obdemo -o system_memory=30G,datafile_size=500G,config_additional_dir=/data/1/obdemo/etc3;/data/log1/obdemo/etc2
   ```

2. Perform the bootstrap operation on the cluster.

   Use OBClient to connect to the started observer process. The password is empty.

   ```shell
   [root@xxx /home/admin]# obclient -h127.1 -uroot -P2881 -p
   Enter password:

   obclient [(none)]> SET SESSION ob_query_timeout=1000000000;
   Query OK, 0 rows affected

   obclient [(none)]> ALTER SYSTEM BOOTSTRAP ZONE 'zone1' SERVER '10.10.10.1:2882';
   Query OK, 0 rows affected
   ```

   <main id="notice" type='notice'>
      <h4>Notice</h4>
      <p>If an error is returned in this step, the reason may be that a startup parameter of the observer process is incorrect, you do not have the required permissions on the related directories of the OBServer node, the disk usage percentage of the log directory is lower than expected, or the memory resource on the node is insufficient. The log directory issue occurs because the log directory shares the same upper-level directory with the data directory and the space is occupied by the data directory. Check these issues. Then, clear the OceanBase Database directory and retry the deployment. </p>
   </main>

3. Verify that the cluster is initialized.

   After you perform the bootstrap operation and execute the `SHOW DATABASES` statement, if `oceanbase` appears in the database list, the cluster is initialized.

   ```shell
   obclient [(none)]> SHOW DATABASES;
   +--------------------+
   | Database           |
   +--------------------+
   | information_schema |
   | LBACSYS            |
   | mysql              |
   | oceanbase          |
   | ORAAUDITOR         |
   | SYS                |
   | test               |
   +--------------------+
   7 rows in set
   ```

4. Change the password.

   By default, the password of the `root` user of the `sys` tenant is empty. After the initialization, change the password.

   ```shell
   obclient [(none)]> ALTER USER root IDENTIFIED BY '******';
   Query OK, 0 rows affected
   ```

## What to do next

After you create a cluster, you can create a user tenant based on business needs.

For more information about how to create a user tenant by using the CLI, see [Create a tenant](../../../../600.manage/200.tenant-management/600.common-tenant-operations/200.manage-create-tenant.md).

## References

* [Connect to an OceanBase Database tenant by using OBClient](../../../../300.develop/100.application-development-of-mysql-mode/100.connect-to-oceanbase-database-of-mysql-mode/300.connect-to-an-oceanbase-tenant-by-using-obclient-of-mysql-mode.md)
* [System parameters](../../../../700.reference/800.configuration-items-and-system-variables/100.system-configuration-items/200.system-configuration-items-overview-list.md)
* [ALTER USER](../../../../700.reference/500.sql-reference/100.sql-syntax/200.common-tenant-of-mysql-mode/600.sql-statement-of-mysql-mode/1800.alter-user-of-mysql-mode.md)
* [User tenants](../../../../600.manage/200.tenant-management/400.introduction-of-user-tenant.md)
* [CREATE TENANT](../../../../700.reference/500.sql-reference/100.sql-syntax/100.system-tenants/800.create-tenant.md)
* [Online upgrade from standalone deployment to distributed deployment](../../../../600.manage/100.cluster-management/300.common-cluster-operations/1400.single-machine-deployment-online-to-distributed.md)