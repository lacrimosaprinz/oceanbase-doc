# Deployment

## What are the system requirements for installing an OceanBase server?

The following table lists the minimum system requirements for installing OceanBase servers.

| **Server type** | **Quantity** | **Minimum functional configuration** | **Minimum performance configuration** |
| --- | --- | --- | --- |
| OceanBase Cloud Platform (OCP) server | 1 | 16 CPU cores, 32 GB memory, and 1.5 TB storage | 32 CPU cores, 128 GB memory, 1.5 TB SSD storage, and 10 Gbit/s NIC |
| OceanBase Database computing server | 3 | 4 CPU cores and 16 GB memory <main id="notice" type='explain'><h4>Note</h4><p>The log disk size must be four times larger than the memory size, and the data disk must be large enough to store target data. </p></main> | 32 CPU cores, 256 GB memory, and 10 Gbit/s NIC <main id="notice" type='explain'><h4>Note</h4><p>The log disk size must be four times larger than the memory size, and the data disk must be large enough to store target data. </p></main>  |

To ensure high availability of the OCP service, deploy OCP on three nodes and use a hardware- or software-based load balancer such as F5, Alibaba Cloud Server Load Balancer (SLB), or OceanBase Database ob_dns.

The following table lists the Linux operating systems that support OceanBase Database.

| Linux operating system | Version | Server architecture |
|-------------------------|-----------|-------------------------------|
| Alibaba Cloud Linux | 2 and 3 | x86_64 or AArch64 |
| AnolisOS | V8.6 and later | x86_64 (including Hygon) or AArch64 (such as Kunpeng and Phytium) |
| KylinOS | V10 | x86_64 (including Hygon) or AArch64 (such as Kunpeng and Phytium) |
| Unity Operating System (UOS) | V20 | x86_64 (including Hygon) or AArch64 (such as Kunpeng and Phytium) |
| NFSChina | V4.0 and later | x86_64 (including Hygon) or AArch64 (such as Kunpeng and Phytium) |
| Inspur KOS | V5.8 | x86_64 (including Hygon) or AArch64 (such as Kunpeng and Phytium) |
| CentOS/Red Hat Enterprise Linux | V7.2 and later | x86_64 (including Hygon) or AArch64 (such as Kunpeng and Phytium) |
| SUSE Enterprise Linux Server | 12SP3 and later | x86_64 (including Hygon) |
| Debian | V8.3 and later | x86_64 (including Hygon) |

<main id="notice" type='explain'>
  <h4>Note</h4>
  <p>Before you use an operating system, configure the network and install a package manager such as YUM or Zypper first. </p>
</main>

## How do I deploy OceanBase Database in the production environment?

The following table describes deployment solutions.

| **Solution** | **Feature** | **Infrastructure requirement** | **Scenario** |
| --- | --- | --- | --- |
| Three replicas in one IDC | This solution provides a zero recovery point objective (RPO) and low recovery time objective (RTO). It also enables automatic failover when the primary server fails.  This solution enables your application to recover from some hardware failures but not IDC failures or city-wide disasters.  | This solution requires a single IDC. | This solution applies to scenarios where you do not expect your application to recover from IDC failures or city-wide disasters.  |
| Three replicas in three IDCs in the same region | This solution provides a zero recovery point objective (RPO) and low recovery time objective (RTO). It also enables automatic failover when the primary server fails.  This solution enables your application to recover from some hardware and IDC failures but not city-wide disasters.  | This solution requires three IDCs in the same region.  Low network latency between the IDCs must be ensured.  | It applies to scenarios where you need your application to recover from IDC failures but do not expect it to recover from city-wide disasters.  |
| Five replicas in five IDCs across three regions | This solution provides a zero recovery point objective (RPO) and low recovery time objective (RTO). It also enables automatic failover when the primary server fails.  This solution enables your application to recover from some hardware failures, IDC failures, and city-wide disasters.  | This solution requires five IDCs across three regions.  Two regions must be geographically close to provide low network latency.  | It applies to scenarios where you need your application to recover from IDC failures and city-wide disasters.  |
| Two IDCs in the same region + Inter-cluster data replication | The RPO of this solution is greater than zero and the RTO is high, but it enables you to determine whether to fail over when the primary server fails.  It enables your application to recover from some hardware and IDC failures but not city-wide disasters.  | This solution requires two IDCs in the same region.  | This solution applies when you have two IDCs in the same region and need your application to recover when either IDC fails.  |
| Five replicas in three IDCs across two regions + Inter-cluster data replication | IDC failure: This solution provides zero RPO and low RTO. It also enables automatic failover when the primary server fails.  City-wide disaster: The RPO of this solution is greater than zero and the RTO is high, but it enables you to determine whether to fail over when the primary server fails.  This solution enables your application to recover from some hardware failures, IDC failures, and city-wide disasters.  | This solution requires three IDCs deployed across two regions.  | This solution applies when you have three IDCs deployed across two regions and you need your application to recover from IDC failures and city-wide disasters.  |
