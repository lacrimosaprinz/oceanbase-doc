# 识别链路上的瓶颈

如 《高可用》章节所述，OceanBase 数据库的数据链路为 APPServer <-> OBProxy <-> OBServer。APPServer 通过数据库驱动连接 ODP 发送请求，由于 OceanBase 数据库的分布式架构，用户数据以多分区多副本的方式分布于多个 OBServer 上，ODP 将用户请求转发到最合适的 OBServer 执行，并将执行结果返回用户。每个 OBServer 也有路由转发的功能，如果发现请求不能在当前节点执行，则会转发请求到正确的 OBServer。

![数据链路图](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.1/manage/odp-data-link2.jpg)

当出现端到端的性能问题时（在数据库场景下，端到端表示在应用服务器上观察到 SQL 请求的 RT 很高），此时首先需要定位是数据库访问链路上哪个组件的问题，再排查组件内的具体问题。

我们一般有两种排查思路：

* **下钻排查法**

  按照数据访问的顺序依次排查链路上各组件，观测该组件调用下游组件的耗时，并继续排查其中耗时明显异常（也称为“耗时热点”）的下游组件。本质是一种 “按图索骥”（递归法）的思路，按照线索一步步下钻，从而逼近根因组件。

* **定向排查法**

  按照历史经验探查最高频异常的组件，观测该组件的核心 SLA 指标以判断该组件是否异常，从而决定下一步探查的组件。本质是一种“排除法”（贪心法）的思路，首先排查最高频组件，并逐步排除，从而逼近根因组件。

  ![定向排查法](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.1/manage/tuning-troubleshoot.png)

上图为简化版的数据库访问链路，值得一提的是，上图中两组件通过网络互相访问时，应该将网络也视为组件，如果怀疑两组件间网络异常，需要排查网络链路上的各级交换机。按照 ODP 的部署形态，对于其中链路较长的网络访问，尤其是跨 VPC/跨云的网络访问，应该保持高度关注。

两种排查思路无绝对优劣，排查人员应该根据手头工具的便利程度，及历史排查经验，选择最高效的排查法：

* 对于首次上线的业务链路，例如某些企业采用混合云部署模式，将部分业务从自建机房迁移到公有云，此时访问链路跨云，适合选择下钻排查法。

* 对于成熟的业务链路，其中某些组件有变更动作，例如大促活动、ODP 升级，适合选择定向排查法。

除了以上介绍的按照场景选择排查法，还可以按照错误提示精确定位到瓶颈组件，该方案非常依赖排查人员的经验。

![定位瓶颈组件](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.1/manage/tuning-app.png)

一个常见的报错堆栈如上图所示，堆栈分为三层，OBServer 和 ODP 不用赘述，我们介绍一下应用服务器部分。应用一般会通过数据中间件对数据库连接进行管理，包括访问鉴权、连接预热、连接池化等功能。数据中间件一般通过包的形式被引入到应用程序中，两者的运行日志一般是分开的。在下文中，我们对应用程序和数据中间件不做区分，统一称为应用层。

每层组件的报错除了在自己的运行日志中打印，还会向上层抛出，并打印在上层组件的运行日志中。所以当某一层出现报错信息时，我们首先应该分析是哪一层的报错：如果是本层组件抛出的错误，应该从本层开始排查。如果是下层组件抛出的错误，可以直接到目标层进行排查。

![应用抖动](https://obbusiness-private.oss-cn-shanghai.aliyuncs.com/doc/img/observer-enterprise/V4.2.1/manage/tuning-app-jitter.png)

下文我们对每一层的常见报错进行介绍，需要说明的是，每一层的报错除了在本层出现，还可能抛向上层并在上层的运行日志中出现：

* **应用报错**

  * 应用报错“数据库连接池满”
  
    该报错是应用系统最常见的报错之一，表示应用系统的数据库连接池满了，新请求取不出连接。应用代码一般开启事务访问数据库，在事务中除了访问数据库，还会有对其他下游系统的调用，例如下游应用系统、缓存等；事务开启时从连接池获取连接，事务结束时归还连接到连接池。连接池满一般是事务耗时太长所致，需要排查事务耗时太长的原因，除了数据库请求 RT 偏高，还可能由于访问其他下游系统耗时太长、应用系统本身的原因等引起。

    * 其他下游系统耗时太长：通过 RPC 调用下游系统的耗时太长，导致事务总体时间变长。其中访问数据库系统的耗时正常。
    
    * 应用系统本身：应用系统本身 fullgc 或者 CPU 被打爆导致的系统卡顿，导致事务总体时间变长。其中访问数据系统的耗时正常。

  * 应用报错“连接数据库失败”
  
    表示应用系统连接 OBServer 失败。可能是后端的数据库系统被请求打爆导致建连失败外，可能是应用系统自身问题（例如发生 fullgc，例如应用服务器网卡异常，例如应用层的数据库配置错误），也可能是应用服务器与 OBServer 之间的网络异常等引起。

  * 应用报错“锁冲突”
  
    表示应用系统对某个数据库对象加锁时发生锁冲突事件。应用系统一般通过锁机制（悲观锁 或者 乐观锁）控制对同一个对象的并发访问。锁冲突具有很强的应用语义，除了数据库系统 RT 飙高导致应用系统大量超时并重试外，需要从应用系统自身寻找原因，例如调度系统异常导致对同一对象大量同时操作，例如某种黑客攻击等。

* **OBProxy 报错**

  * OBProxy 报错 `ERROR 1317 (70100): Query execution was interrupted`
  
    表示客户端主动中断了查询，一般是客户端发送的 SQL 请求在预期超时时间内未返回；或者用户通过 `Ctrl + C` 主动发起的 `Kill Query` 行为。当在生产环境中大量出现该报错时，表示在应用视角的 SQL 耗时超出了客户端超时时间（Query Timeout），需要排查客户端到 OBserver 之间的每一个组件。

* **OBServer 报错**

  * OBServer报错 `ERROR 4012 (HY000): Timeout, query has reached the maximum query timeout`
  
    表示 SQL 请求在 OBServer 内部处理时间超过了服务端超时时间（`ob_query_timeout`），可以直接定位为 OBServer 内部存在异常。

    `ob_query_timeout` 用于设置 SQL 最大执行时间，单位是微秒。有关该变量的详细信息，参见 [ob_query_timeout](../../700.reference/800.configuration-items-and-system-variables/200.system-variable/300.global-system-variable/9000.ob_query_timeout-global.md)。

<main id="notice" type='notice'>
    <h4>注意</h4>
    <p>以上介绍的按照报错提示精确定位到组件，可能因各组件的版本不同而有所差异，非常依靠排查人员的经验积累。排查人员应该根据特定版本积累一套排查方法论。</p>
</main>
